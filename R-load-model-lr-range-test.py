import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers

import numpy as np
import pandas as pd
import os
import time


ENCODING = "ISO-8859-2"

basedir = '/media/jason/Games/ml-data/trump-change'
datadir = os.path.join(basedir, 'data')
filepath = os.path.join(datadir, 'trump-tweets-no-retweets.json') #csv'

df = train = pd.read_json(filepath)
corpus = train['text'].values # .astype(str) # conversion causing encoding issues?

# CHARSETS MUST BE TRACKED ACROSS MODELS.  INVALIDATES OLD MODELS IF CHANGED
# Remove unusual chars to decrease vocab size
chars_to_rm = np.array(['ğŸ‡¦', 'ğŸ‡§', 'ğŸ‡¨', 'ğŸ‡ª', 'ğŸ‡«', 'ğŸ‡¬', 'ğŸ‡®', 'ğŸ‡¯', 'ğŸ‡°', 'ğŸ‡±', 'ğŸ‡²', 'ğŸ‡³', 'ğŸ‡´',
       'ğŸ‡µ', 'ğŸ‡·', 'ğŸ‡¸', 'ğŸ‡¹', 'ğŸ‡º', 'ğŸ‡½', 'ğŸŒŠ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒª', 'ğŸŒ²', 'ğŸ¾', 'ğŸ¿',
       'ğŸ‚', 'ğŸ„', 'ğŸ†', 'ğŸ‰', 'ğŸ¥', 'ğŸ', 'ğŸ†', 'ğŸˆ', 'ğŸ»', 'ğŸ¼', 'ğŸ½', 'ğŸ‘€', 'ğŸ‘‡',  'ğŸ¤³', 'ğŸ¦…',
       'ğŸ‘ˆ', 'ğŸ‘‰', 'ğŸ‘Š', 'ğŸ‘', 'ğŸ‘', 'ğŸ‘·', 'ğŸ’œ', 'ğŸ’¤', 'ğŸ’¥', 'ğŸ’ª', 'ğŸ’«', 'ğŸ’¬', 'ğŸ’¯',
       'ğŸ’°', 'ğŸ“ˆ', 'ğŸ“‰', 'ğŸ“Œ', 'ğŸ“', 'ğŸ“±', 'ğŸ“¸', 'ğŸ“º', 'ğŸ”Ÿ', 'ğŸ”¥', 'ğŸ”´', 'ğŸ”¹', 'ğŸ˜‚',
       'ğŸ˜†', 'ğŸ˜‰', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ˜³', 'ğŸ™„', 'ğŸ™‹', 'ğŸ™Œ', 'ğŸ™', 'ğŸš€', 'ğŸš’', 'ğŸšš', 'ğŸš›',
       'ğŸš¨', 'ğŸš«', 'ğŸ›’', 'ğŸ›°', 'ğŸ¤”', 'ğŸ¤—', 'ğŸ¤¡', 'ğŸ¤£', 'ğŸ¤¦', 'â‚¬', 'âƒ£', 'â„¢', 'â†’', 'â†“', 
       'â†´', 'â‡’', 'â°', 'â–ª', 'â–¶', 'â—', 'â—¦', 'â˜€', 'â˜…', 'â˜‘', 'â˜˜', 'â™€', 'â™‚', 'âš–', 'âš™', 
       'âš ', 'âš¡', 'âš½', 'âš¾', 'â›ˆ', 'â›°', 'â›³', 'âœ…', 'âœˆ', 'âœ', 'âœ“', 'âœ”', 'âœ–', 'âŒ', 
       'â—', 'â£', 'â¤', 'âœ', 'â¡', 'â¬†', 'â¬‡', 'â­', 'ã€', 'ã€‚', 'ã€…', 'ã€Š', 'ã€Œ', 'ã€', 
       'ã€', 'ã€‘', 'ã„', 'ã†', 'ãˆ', 'ãŠ', 'ã‹', # New chars to remove above this line 
       'ãŒ', 'ã', 'ã', 'ã', 'ã‘', 'ã“', 'ã”', 'ã•', 'ã—', 'ã˜', 'ã™', 'ã', 'ãŸ',
       'ã ', 'ã£', 'ã¤', 'ã¦', 'ã§', 'ã¨', 'ãª', 'ã«', 'ã®', 'ã¯', 'ã¸', 'ã¾', 'ã¿',
       'ã‚', 'ã‚‚', 'ã‚ˆ', 'ã‚‰', 'ã‚Š', 'ã‚‹', 'ã‚’', 'ã‚¢', 'ã‚´', 'ã‚µ', 'ã‚¸', 'ã‚»', 'ãƒƒ',
       'ãƒˆ', 'ãƒ‹', 'ãƒ•', 'ãƒ—', 'ãƒŸ', 'ãƒ¡', 'ãƒ©', 'ãƒ«', 'ãƒ³', 'ãƒ¼', 'ä¸Š', 'ä¸‹', 'ä¸–',
       'ä¸¡', 'äºº', 'ä»£', 'ä»¤', 'ä»¥', 'ä¼š', 'å€', 'å…±', 'å…¸', 'å‡º', 'åˆ', 'åŠ±', 'å‹¢',
       'åŒ—', 'åƒ', 'å²', 'åŒ', 'å', 'å’Œ', 'å•', 'å›½', 'å¤•', 'å¤§', 'å¤©', 'å¤«', 'å®‰',
       'å¯¾', 'å±…', 'å¸­', 'å¼', 'å¿œ', 'æƒ…', 'æŠ•', 'æƒ', 'æº', 'æ–°', 'æ—¥', 'æ˜¨', 'æ™‚',
       'æ›´', 'æœ', 'æœ¬', 'æ§˜', 'æ­“', 'æµ·', 'æ¸ˆ', 'æ¿€', 'ç†', 'ç•Œ', 'çš‡', 'ç›Ÿ', 'ç›¸',
       'ç¨¿', 'ç±³', 'çµŒ', 'çµ±', 'ç¶š', 'ç·', 'è€ƒ', 'è„³', 'è‡¨', 'è‡ª', 'è‡³', 'è‰¦', 'è‘‰',
       'è¡›', 'è¦‹', 'è¨ª', 'è©±', 'èª²', 'è«‡', 'è­·', 'è³“', 'è»', 'è¿', 'é', 'é–“', 'é˜ª',
       'é™›', 'éšŠ', 'é ˜', 'é¡Œ', 'é£Ÿ', 'é¦–', 'é®®', 'ê³ ', 'êµ­', 'ëŠ”', 'ëŒ€', 'ë¼', 'ë ›',
       'ë¦¬', 'ë©°', 'ëª¨', 'ë¯¸', 'ë°”', 'ë°›', 'ë³´', 'ë¶', 'ë¸Œ', 'ìƒ', 'ì„œ', 'ì†Œ', 'ìŠµ',
       'ì—', 'ì˜¤', 'ìš¸', 'ì„', 'ì˜', 'ìŸ', 'ì „', 'ì •', 'ì´ˆ', 'ì¸¡', 'í•‘', 'í•˜', 'í•œ',
       'í™”', '\u200b', '\u200c', '\u200d', '\u200e', '\u200f', 'â€“', 'â€”',
       'â€•', 'â€˜', 'â€™', 'â€œ', 'â€', 'â€¢', 'â€¦', '\u202f', 'â€²', 'â€¼', '\u2060',
       '\u2063', '\u2066', '\u2069', '\U0001f928', '\U0001f92f',
       '\U0001f973', '\U0001f9d0', '\U0010fc00',
       '~', 'Â¡', 'Â£', 'Â«', 'Â®', 'Âº', 'Â»', 'Â½', 'Ã‰', 'Ã—', 'Ã¡', 'Ã¢', 'Ã©',
       'Ã­', 'Ã¯', 'Ã±', 'Ã³', 'Ã´', 'Ã¶', 'Ã¸', 'Ãº', 'ÄŸ', 'Å', 'Éª', 'É´', 'Ê€',
       '×', '×‘', '×’', '×“', '×”', '×•', '×–', '×—', '×˜', '×™', '×š', '×›', '×œ',
       '×', '×', '×Ÿ', '× ', '×¡', '×¢', '×¦', '×§', '×¨', '×©', '×ª', 'ØŒ', 'Ø¡',
       'Ø¢', 'Ø£', 'Ø¤', 'Ø¥', 'Ø§', 'Ø¨', 'Ø©', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯',
       'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´', 'Øµ', 'Ø¶', 'Ø·', 'Ø¸', 'Ø¹', 'Øº', 'Ù', 'Ù‚',
       'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'ÙŠ', 'Ù‹', 'Ú†', 'Ú˜', 'Ú©', 'Ú¯', 'ÛŒ',
       'Û°', 'Û´', 'à¤', 'à¤‚', 'à¤…', 'à¤†', 'à¤‡', 'à¤‰', 'à¤', 'à¤”', 'à¤•', 'à¤–', 'à¤—',
       'à¤˜', 'à¤š', 'à¤›', 'à¤œ', 'à¤', 'à¤Ÿ', 'à¤ ', 'à¤¡', 'à¤£', 'à¤¤', 'à¤¥', 'à¤¦', 'à¤§',
       'à¤¨', 'à¤ª', 'à¤¬', 'à¤­', 'à¤®', 'à¤¯', 'à¤°', 'à¤²', 'à¤µ', 'à¤¶', 'à¤·', 'à¤¸', 'à¤¹',
       'à¤¼', 'à¤¾', 'à¤¿', 'à¥€', 'à¥', 'à¥‚', 'à¥‡', 'à¥ˆ', 'à¥‹', 'à¥', 'à¥¤', 'á´…', 'á´‡',
       'á´', 'á»…', 'ï¸'])



import string, re # gruber html regex
regex = pat = r'\b(([\w-]+://?|www[.])[^\s()<>]+(?:\([\w\d]+\)|([^%s\s]|/)))'
regex = pat = pat % re.sub(r'([-\\\]])', r'\\\1', string.punctuation)

corpus_no_html = np.array(list(map(lambda x: re.sub(regex, ' ', x), corpus)))



s = ''
for l in corpus_no_html: 
    s += (l + ' ') # (l + '\n') or (l + ' \n ')
print(s[:280])



# Remove special chars
s_clean = ''
for c in s:
    if c not in chars_to_rm:
        s_clean += c + ''
print(s_clean[:280])


text = s_clean


# length of text is the number of characters in it
print ('Length of text: {} characters'.format(len(text)))

# Take a look at the first 250 characters in text
print(text[:250])


# The unique characters in the file
vocab = sorted(set(text))
print ('{} unique characters'.format(len(vocab)))



# Creating a mapping from unique characters to indices
char2idx = {u:i for i, u in enumerate(vocab)}
idx2char = np.array(vocab)

text_as_int = np.array([char2idx[c] for c in text])

# Now we have an integer representation for each character. 
# Notice that we mapped the character as indexes from 0 to len(unique).
print('{')
for char,_ in zip(char2idx, range(20)):
    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))
print('  ...\n}')



# Show how the first 13 characters from the text are mapped to integers
print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))


# The maximum length sentence we want for a single input in characters
seq_length = max([len(x) for x in corpus_no_html]) # 144 OG tweet length
examples_per_epoch = len(text) // (seq_length + 1)

# Create training examples / targets
char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)


for i in char_dataset.take(5):
    print(idx2char[i.numpy()])


sequences = char_dataset.batch(seq_length+1, drop_remainder=True)

for item in sequences.take(5):
    print(repr(''.join(idx2char[item.numpy()])))

def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text, target_text


dataset = sequences.map(split_input_target)


# Print the first examples input and target values
for input_example, target_example in  dataset.take(1):
    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))
    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))

for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):
    print("Step {:4d}".format(i))
    print("  input: {} ({:s})".format(input_idx, repr(idx2char[input_idx])))
    print("  expected output: {} ({:s})".format(target_idx, repr(idx2char[target_idx])))


# Batch size
BATCH_SIZE = 136 # close to a divisor of examples_per_epoch

# Buffer size to shuffle the dataset
# (TF data is designed to work with possibly infinite sequences,
# so it doesn't attempt to shuffle the entire sequence in memory. Instead,
# it maintains a buffer in which it shuffles elements).
BUFFER_SIZE = 10000

dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

print(dataset)



## Build model

# Length of the vocabulary in chars
vocab_size = len(vocab)

# The embedding dimension
embedding_dim = 206 # divisor of examples_per_epoch

# Number of RNN units
rnn_units = 412 # divisor **


def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
  model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim,
                              batch_input_shape=[batch_size, None]),
    tf.keras.layers.GRU(rnn_units,
                        return_sequences=True,
                        stateful=True,
                        recurrent_initializer='glorot_uniform'),
    tf.keras.layers.GRU(rnn_units,
                        return_sequences=True,
                        stateful=True,
                        recurrent_initializer='glorot_uniform'),
    tf.keras.layers.GRU(rnn_units,
                        return_sequences=True,
                        stateful=True,
                        recurrent_initializer='glorot_uniform'),
    # tf.keras.layers.Dense(rnn_units // 2, activation='relu'),
    tf.keras.layers.Dense(vocab_size*2, activation='relu'),
    tf.keras.layers.Dense(vocab_size)
  ])
  return model

model = build_model(
  vocab_size = len(vocab),
  embedding_dim=embedding_dim,
  rnn_units=rnn_units,
  batch_size=BATCH_SIZE)

print(model.summary())


# Try the model
for input_example_batch, target_example_batch in dataset.take(1):
  example_batch_predictions = model(input_example_batch)
  print(example_batch_predictions.shape, "# (batch_size, sequence_length, vocab_size)")


# First example in batch
sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)
sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()
print(sampled_indices)


print("Input: \n", repr("".join(idx2char[input_example_batch[0]])))
print()
print("Next Char Predictions: \n", repr("".join(idx2char[sampled_indices])))


print("Input: \n", repr("".join(idx2char[input_example_batch[0]])))
print()
print("Next Char Predictions: \n", repr("".join(idx2char[sampled_indices ])))


# Train the model
def loss_fn(labels, logits):
    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)

example_batch_loss  = loss_fn(target_example_batch, example_batch_predictions)
print("Prediction shape: ", example_batch_predictions.shape, " # (batch_size, sequence_length, vocab_size)")
print("scalar_loss:      ", example_batch_loss.numpy().mean())

model.compile(optimizer='adam', loss=loss_fn)
