
# Ref:  https://www.tensorflow.org/tutorials/text/text_generation
import tensorflow as tf

import numpy as np
import pandas as pd

import os
import time
import yaml

# Import flags if not in namespace?
# Load flags.yaml
stream = open("flags.yaml", 'r')
FLAGS = yaml.load(stream)



ENCODING = FLAGS['encoding'] #"ISO-8859-2"

# basedir = '/home/jason/Documents'
basedir = '/media/jason/freya/ml-data/trump-change'
datadir = os.path.join(basedir, 'data')
filepath = os.path.join(datadir, 'trump-tweets-no-retweets.json') #csv'


df = train = pd.read_json(filepath)#, encoding=ENCODING)
# df = train = pd.read_csv(filepath)
corpus = train['text'].values # .astype(str) # conversion causing encoding issues?
corpus = corpus[:100]

# CHARSETS MUST BE TRACKED ACROSS MODELS.  INVALIDATES OLD MODELS IF CHANGED
# Remove unusual chars to decrease vocab size
chars_to_rm = np.array(['ğŸ‡¦', 'ğŸ‡§', 'ğŸ‡¨', 'ğŸ‡ª', 'ğŸ‡«', 'ğŸ‡¬', 'ğŸ‡®', 'ğŸ‡¯', 'ğŸ‡°', 'ğŸ‡±', 'ğŸ‡²', 'ğŸ‡³', 'ğŸ‡´',
       'ğŸ‡µ', 'ğŸ‡·', 'ğŸ‡¸', 'ğŸ‡¹', 'ğŸ‡º', 'ğŸ‡½', 'ğŸŒŠ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒª', 'ğŸŒ²', 'ğŸ¾', 'ğŸ¿',
       'ğŸ‚', 'ğŸ„', 'ğŸ†', 'ğŸ‰', 'ğŸ¥', 'ğŸ', 'ğŸ†', 'ğŸˆ', 'ğŸ»', 'ğŸ¼', 'ğŸ½', 'ğŸ‘€', 'ğŸ‘‡',  'ğŸ¤³', 'ğŸ¦…',
       'ğŸ‘ˆ', 'ğŸ‘‰', 'ğŸ‘Š', 'ğŸ‘', 'ğŸ‘', 'ğŸ‘·', 'ğŸ’œ', 'ğŸ’¤', 'ğŸ’¥', 'ğŸ’ª', 'ğŸ’«', 'ğŸ’¬', 'ğŸ’¯',
       'ğŸ’°', 'ğŸ“ˆ', 'ğŸ“‰', 'ğŸ“Œ', 'ğŸ“', 'ğŸ“±', 'ğŸ“¸', 'ğŸ“º', 'ğŸ”Ÿ', 'ğŸ”¥', 'ğŸ”´', 'ğŸ”¹', 'ğŸ˜‚',
       'ğŸ˜†', 'ğŸ˜‰', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ˜³', 'ğŸ™„', 'ğŸ™‹', 'ğŸ™Œ', 'ğŸ™', 'ğŸš€', 'ğŸš’', 'ğŸšš', 'ğŸš›',
       'ğŸš¨', 'ğŸš«', 'ğŸ›’', 'ğŸ›°', 'ğŸ¤”', 'ğŸ¤—', 'ğŸ¤¡', 'ğŸ¤£', 'ğŸ¤¦', 'â‚¬', 'âƒ£', 'â„¢', 'â†’', 'â†“', 
       'â†´', 'â‡’', 'â°', 'â–ª', 'â–¶', 'â—', 'â—¦', 'â˜€', 'â˜…', 'â˜‘', 'â˜˜', 'â™€', 'â™‚', 'âš–', 'âš™', 
       'âš ', 'âš¡', 'âš½', 'âš¾', 'â›ˆ', 'â›°', 'â›³', 'âœ…', 'âœˆ', 'âœ', 'âœ“', 'âœ”', 'âœ–', 'âŒ', 
       'â—', 'â£', 'â¤', 'âœ', 'â¡', 'â¬†', 'â¬‡', 'â­', 'ã€', 'ã€‚', 'ã€…', 'ã€Š', 'ã€Œ', 'ã€', 
       'ã€', 'ã€‘', 'ã„', 'ã†', 'ãˆ', 'ãŠ', 'ã‹', # New chars to remove above this line 
       'ãŒ', 'ã', 'ã', 'ã', 'ã‘', 'ã“', 'ã”', 'ã•', 'ã—', 'ã˜', 'ã™', 'ã', 'ãŸ',
       'ã ', 'ã£', 'ã¤', 'ã¦', 'ã§', 'ã¨', 'ãª', 'ã«', 'ã®', 'ã¯', 'ã¸', 'ã¾', 'ã¿',
       'ã‚', 'ã‚‚', 'ã‚ˆ', 'ã‚‰', 'ã‚Š', 'ã‚‹', 'ã‚’', 'ã‚¢', 'ã‚´', 'ã‚µ', 'ã‚¸', 'ã‚»', 'ãƒƒ',
       'ãƒˆ', 'ãƒ‹', 'ãƒ•', 'ãƒ—', 'ãƒŸ', 'ãƒ¡', 'ãƒ©', 'ãƒ«', 'ãƒ³', 'ãƒ¼', 'ä¸Š', 'ä¸‹', 'ä¸–',
       'ä¸¡', 'äºº', 'ä»£', 'ä»¤', 'ä»¥', 'ä¼š', 'å€', 'å…±', 'å…¸', 'å‡º', 'åˆ', 'åŠ±', 'å‹¢',
       'åŒ—', 'åƒ', 'å²', 'åŒ', 'å', 'å’Œ', 'å•', 'å›½', 'å¤•', 'å¤§', 'å¤©', 'å¤«', 'å®‰',
       'å¯¾', 'å±…', 'å¸­', 'å¼', 'å¿œ', 'æƒ…', 'æŠ•', 'æƒ', 'æº', 'æ–°', 'æ—¥', 'æ˜¨', 'æ™‚',
       'æ›´', 'æœ', 'æœ¬', 'æ§˜', 'æ­“', 'æµ·', 'æ¸ˆ', 'æ¿€', 'ç†', 'ç•Œ', 'çš‡', 'ç›Ÿ', 'ç›¸',
       'ç¨¿', 'ç±³', 'çµŒ', 'çµ±', 'ç¶š', 'ç·', 'è€ƒ', 'è„³', 'è‡¨', 'è‡ª', 'è‡³', 'è‰¦', 'è‘‰',
       'è¡›', 'è¦‹', 'è¨ª', 'è©±', 'èª²', 'è«‡', 'è­·', 'è³“', 'è»', 'è¿', 'é', 'é–“', 'é˜ª',
       'é™›', 'éšŠ', 'é ˜', 'é¡Œ', 'é£Ÿ', 'é¦–', 'é®®', 'ê³ ', 'êµ­', 'ëŠ”', 'ëŒ€', 'ë¼', 'ë ›',
       'ë¦¬', 'ë©°', 'ëª¨', 'ë¯¸', 'ë°”', 'ë°›', 'ë³´', 'ë¶', 'ë¸Œ', 'ìƒ', 'ì„œ', 'ì†Œ', 'ìŠµ',
       'ì—', 'ì˜¤', 'ìš¸', 'ì„', 'ì˜', 'ìŸ', 'ì „', 'ì •', 'ì´ˆ', 'ì¸¡', 'í•‘', 'í•˜', 'í•œ',
       'í™”', '\u200b', '\u200c', '\u200d', '\u200e', '\u200f', 'â€“', 'â€”',
       'â€•', 'â€˜', 'â€™', 'â€œ', 'â€', 'â€¢', 'â€¦', '\u202f', 'â€²', 'â€¼', '\u2060',
       '\u2063', '\u2066', '\u2069', '\U0001f928', '\U0001f92f',
       '\U0001f973', '\U0001f9d0', '\U0010fc00',
       '~', 'Â¡', 'Â£', 'Â«', 'Â®', 'Âº', 'Â»', 'Â½', 'Ã‰', 'Ã—', 'Ã¡', 'Ã¢', 'Ã©',
       'Ã­', 'Ã¯', 'Ã±', 'Ã³', 'Ã´', 'Ã¶', 'Ã¸', 'Ãº', 'ÄŸ', 'Å', 'Éª', 'É´', 'Ê€',
       '×', '×‘', '×’', '×“', '×”', '×•', '×–', '×—', '×˜', '×™', '×š', '×›', '×œ',
       '×', '×', '×Ÿ', '× ', '×¡', '×¢', '×¦', '×§', '×¨', '×©', '×ª', 'ØŒ', 'Ø¡',
       'Ø¢', 'Ø£', 'Ø¤', 'Ø¥', 'Ø§', 'Ø¨', 'Ø©', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯',
       'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´', 'Øµ', 'Ø¶', 'Ø·', 'Ø¸', 'Ø¹', 'Øº', 'Ù', 'Ù‚',
       'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'ÙŠ', 'Ù‹', 'Ú†', 'Ú˜', 'Ú©', 'Ú¯', 'ÛŒ',
       'Û°', 'Û´', 'à¤', 'à¤‚', 'à¤…', 'à¤†', 'à¤‡', 'à¤‰', 'à¤', 'à¤”', 'à¤•', 'à¤–', 'à¤—',
       'à¤˜', 'à¤š', 'à¤›', 'à¤œ', 'à¤', 'à¤Ÿ', 'à¤ ', 'à¤¡', 'à¤£', 'à¤¤', 'à¤¥', 'à¤¦', 'à¤§',
       'à¤¨', 'à¤ª', 'à¤¬', 'à¤­', 'à¤®', 'à¤¯', 'à¤°', 'à¤²', 'à¤µ', 'à¤¶', 'à¤·', 'à¤¸', 'à¤¹',
       'à¤¼', 'à¤¾', 'à¤¿', 'à¥€', 'à¥', 'à¥‚', 'à¥‡', 'à¥ˆ', 'à¥‹', 'à¥', 'à¥¤', 'á´…', 'á´‡',
       'á´', 'á»…', 'ï¸'])



import string, re # gruber html regex
regex = pat = r'\b(([\w-]+://?|www[.])[^\s()<>]+(?:\([\w\d]+\)|([^%s\s]|/)))'
regex = pat = pat % re.sub(r'([-\\\]])', r'\\\1', string.punctuation)

corpus_no_html = np.array(list(map(lambda x: re.sub(regex, ' ', x), corpus)))



s = ''
for l in corpus_no_html: 
    s += (l + ' ') # (l + '\n') or (l + ' \n ')
print(s[:280])



# Remove special chars
s_clean = ''
for c in s:
    if c not in chars_to_rm:
        s_clean += c + ''
print(s_clean[:280])


text = s_clean


# length of text is the number of characters in it
print ('Length of text: {} characters'.format(len(text)))

# Take a look at the first 250 characters in text
print(text[:250])


# The unique characters in the file
vocab = sorted(set(text))
print ('{} unique characters'.format(len(vocab)))



# Creating a mapping from unique characters to indices
char2idx = {u:i for i, u in enumerate(vocab)}
idx2char = np.array(vocab)

text_as_int = np.array([char2idx[c] for c in text])

if FLAGS['verbose']:
    # Now we have an integer representation for each character. 
    # Notice that we mapped the character as indexes from 0 to len(unique).
    print('{')
    for char,_ in zip(char2idx, range(20)):
        print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))
    print('  ...\n}')

    # Show how the first 13 characters from the text are mapped to integers
    print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))


# The maximum length sentence we want for a single input in characters
seq_length = max([len(x) for x in corpus_no_html]) # 144 OG tweet length
examples_per_epoch = len(text) // (seq_length + 1)

# Create training examples / targets
char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)

sequences = char_dataset.batch(seq_length+1, drop_remainder=True)

if FLAGS['verbose']:
    for i in char_dataset.take(5):
        print(idx2char[i.numpy()])

    for item in sequences.take(5):
        print(repr(''.join(idx2char[item.numpy()])))


def split_input_target(chunk):
    input_text = chunk[:-1]
    target_text = chunk[1:]
    return input_text, target_text


dataset = sequences.map(split_input_target)


if FLAGS['verbose']:
# Print the first examples input and target values
    for input_example, target_example in  dataset.take(1):
        print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))
        print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))

    for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):
        print("Step {:4d}".format(i))
        print("  input: {} ({:s})".format(input_idx, repr(idx2char[input_idx])))
        print("  expected output: {} ({:s})".format(target_idx, repr(idx2char[target_idx])))


# Batch size
BATCH_SIZE = FLAGS['batch_size'] # close to a divisor of examples_per_epoch

# Buffer size to shuffle the dataset
# (TF data is designed to work with possibly infinite sequences,
# so it doesn't attempt to shuffle the entire sequence in memory. Instead,
# it maintains a buffer in which it shuffles elements).
BUFFER_SIZE = FLAGS['buffer_size']

dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

print(dataset)


# Write vocab file out
np.save(os.path.join('data', FLAGS['vocab_file']), vocab)

# Save metadata to yaml/json?
